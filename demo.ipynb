{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0CfJyM77HeOi"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPQMMW9N1viUjx8Cv4J2uGP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LG Pilot Demo"
      ],
      "metadata": {
        "id": "NK-5ziNEESfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch PyMuPDF python-docx python-pptx konlpy rank-bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_E3ovhuFkCE",
        "outputId": "c3efe02f-fb56-4dd0-abb3-ffb070b1bd4b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.5-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting python-pptx\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting rank-bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.12/dist-packages (from python-pptx) (11.3.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Downloading pymupdf-1.26.5-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (495 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m495.9/495.9 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: XlsxWriter, rank-bm25, python-docx, PyMuPDF, JPype1, python-pptx, konlpy\n",
            "Successfully installed JPype1-1.6.0 PyMuPDF-1.26.5 XlsxWriter-3.2.9 konlpy-0.6.0 python-docx-1.2.0 python-pptx-1.0.2 rank-bm25-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import re\n",
        "import os\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "# PDF 처리\n",
        "import fitz\n",
        "# DOCX 처리\n",
        "from docx import Document\n",
        "# PPTX 처리\n",
        "from pptx import Presentation\n",
        "from konlpy.tag import Okt\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. 모델/토크나이저 초기화 및 환경 설정\n",
        "# ==============================================================================\n",
        "\n",
        "model_name = \"LGAI-EXAONE/EXAONE-4.0-1.2B\"\n",
        "model = None\n",
        "tokenizer = None\n",
        "okt = Okt() # KonLPy Okt 초기화\n",
        "\n",
        "def initialize_llm(model_name: str):\n",
        "    \"\"\"LLM 모델과 토크나이저를 로드하고 전역 변수에 할당합니다.\"\"\"\n",
        "    global model, tokenizer\n",
        "    try:\n",
        "        print(f\"Loading model: {model_name}...\")\n",
        "        # ⭐ GPU 환경에서 실행을 위해 device_map=\"auto\"와 dtype=\"bfloat16\" 사용\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            dtype=torch.bfloat16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        print(\"Model and Tokenizer loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        model, tokenizer = None, None\n",
        "        print(\"FATAL: Model loading failed. LLM generation will be impossible.\")\n",
        "\n",
        "def tokenize_ko_en(text: str) -> List[str]:\n",
        "    \"\"\"한글은 형태소 분석(스테밍 포함), 영어는 소문자화 및 공백 분리하여 토큰 반환\"\"\"\n",
        "    # 한글 형태소 분석\n",
        "    ko_tokens = okt.morphs(text, stem=True)\n",
        "\n",
        "    # 영어 및 기타 텍스트 처리 (형태소 분석 결과에 포함되지 않은 토큰)\n",
        "    text_words = set(text.split())\n",
        "\n",
        "    en_tokens = [\n",
        "        token.lower()\n",
        "        for token in text_words\n",
        "        if token.lower() not in ko_tokens and token.isalnum() # 순수 영어 단어 추출 시 유효성 검사 추가\n",
        "    ]\n",
        "    return ko_tokens + en_tokens"
      ],
      "metadata": {
        "id": "bGc81CFoZobM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 2. 문서 처리 함수\n",
        "# ==============================================================================\n",
        "\n",
        "def parse_pdf(file_path: str) -> List[Dict]:\n",
        "    \"\"\"PDF 파일을 파싱하여 페이지별 텍스트를 리스트로 반환합니다.\"\"\"\n",
        "    pages_data = []\n",
        "    try:\n",
        "        doc = fitz.open(file_path)\n",
        "        for page_num in range(doc.page_count):\n",
        "            page = doc.load_page(page_num)\n",
        "            text = page.get_text(\"text\")\n",
        "            if text.strip():\n",
        "                pages_data.append({'type': 'page', 'index': page_num + 1, 'content': text.strip()})\n",
        "        doc.close()\n",
        "    except Exception as e:\n",
        "        print(f\"PDF Parsing Error in {file_path}: {e}\")\n",
        "    return pages_data\n",
        "\n",
        "def parse_pptx(file_path: str) -> List[Dict]:\n",
        "    \"\"\"PPTX 파일을 파싱하여 슬라이드별 텍스트를 리스트로 반환합니다.\"\"\"\n",
        "    slides_data = []\n",
        "    try:\n",
        "        prs = Presentation(file_path)\n",
        "        for slide_num, slide in enumerate(prs.slides):\n",
        "            slide_text = []\n",
        "            for shape in slide.shapes:\n",
        "                if hasattr(shape, \"text\") and shape.has_text_frame:\n",
        "                    slide_text.append(shape.text)\n",
        "\n",
        "            content = \"\\n\".join(t.strip() for t in slide_text if t.strip())\n",
        "            if content:\n",
        "                 slides_data.append({'type': 'slide', 'index': slide_num + 1, 'content': content})\n",
        "    except Exception as e:\n",
        "        print(f\"PPTX Parsing Error in {file_path}: {e}\")\n",
        "    return slides_data\n",
        "\n",
        "def parse_docx(file_path: str) -> List[Dict]:\n",
        "    \"\"\"DOCX 파일을 파싱하여 단락별 텍스트를 리스트로 반환합니다.\"\"\"\n",
        "    paragraphs_data = []\n",
        "    try:\n",
        "        doc = Document(file_path)\n",
        "        for para_num, paragraph in enumerate(doc.paragraphs):\n",
        "            text = paragraph.text.strip()\n",
        "            if text:\n",
        "                paragraphs_data.append({'type': 'paragraph', 'index': para_num + 1, 'content': text})\n",
        "    except Exception as e:\n",
        "        print(f\"DOCX Parsing Error in {file_path}: {e}\")\n",
        "    return paragraphs_data\n",
        "\n",
        "def parse_folder_documents(folder_path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"폴더 내의 지원되는 문서 파일들을 파싱하여 '문서 객체 리스트' 형태로 반환합니다.\"\"\"\n",
        "    all_documents_list: List[Dict[str, Any]] = []\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"Error: Folder path does not exist: {folder_path}\")\n",
        "        return []\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        if os.path.isfile(file_path):\n",
        "            extension = filename.split('.')[-1].lower()\n",
        "            parsed_data: List[Dict] = []\n",
        "\n",
        "            try:\n",
        "                if extension == 'pdf':\n",
        "                    parsed_data = parse_pdf(file_path)\n",
        "                elif extension == 'pptx':\n",
        "                    parsed_data = parse_pptx(file_path)\n",
        "                elif extension == 'docx':\n",
        "                    parsed_data = parse_docx(file_path)\n",
        "\n",
        "                if parsed_data:\n",
        "                    # 문서 단위 객체로 구성\n",
        "                    all_documents_list.append({\n",
        "                        'filename': filename,\n",
        "                        'passages': parsed_data\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "    return all_documents_list"
      ],
      "metadata": {
        "id": "hr4RujmNWaGV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 3. RAG/BM25 및 LLM 관련 함수\n",
        "# ==============================================================================\n",
        "\n",
        "def call_llm_generate(prompt: str, max_new_tokens: int) -> str:\n",
        "    \"\"\"LLM을 호출하고 응답 텍스트만 반환합니다. 모델 로드 실패 시 예외를 발생시킵니다.\"\"\"\n",
        "    global model, tokenizer\n",
        "    if model is None or tokenizer is None:\n",
        "        raise RuntimeError(\"FATAL ERROR: LLM Model or Tokenizer failed to load. Cannot perform generation.\")\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device) # 입력 텐서를 모델 디바이스로 이동\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response_raw = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    tag = \"</think>\"\n",
        "    if tag in response_raw:\n",
        "        return response_raw\n",
        "    else:\n",
        "        # 모델이 프롬프트만 반환하는 경우, 전체를 반환\n",
        "        return response_raw\n",
        "\n",
        "def retrieve_and_augment_by_file(query: str, file_passages: List[Dict], file_name: str, N: int = 5):\n",
        "    \"\"\"\n",
        "    특정 파일 내의 조각(Passages)에 대해서만 BM25 검색을 수행하고 1차 프롬프트를 구성합니다.\n",
        "    \"\"\"\n",
        "    if not file_passages:\n",
        "        return f\"문서 '{file_name}'에는 내용이 없습니다.\", None, None\n",
        "\n",
        "    # 코퍼스 토큰화 및 BM25 인덱스 생성\n",
        "    tokenized_corpus = [tokenize_ko_en(p['content']) for p in file_passages]\n",
        "    bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "    # 질문 토큰화 및 점수 계산\n",
        "    tokenized_query = tokenize_ko_en(query)\n",
        "    doc_scores = bm25.get_scores(tokenized_query)\n",
        "\n",
        "    # 관련성 높은 상위 N개 인덱스 추출\n",
        "    top_n_indices = doc_scores.argsort()[::-1][:N]\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"🔎 BM25 검색 시작 (문서: {file_name})\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # 문서 내용 포맷팅\n",
        "    document_parts = []\n",
        "\n",
        "    for rank_idx, doc_index in enumerate(top_n_indices):\n",
        "        passage_info = file_passages[doc_index]\n",
        "        doc_content = passage_info['content'].replace('\\n', ' ')\n",
        "        document_parts.append(f\"번호 {rank_idx + 1}. : {doc_content}\")\n",
        "        # 디버깅용 출력은 생략하고 프롬프트에 집중\n",
        "        print(f\"번호 {rank_idx + 1}. 페이지 {passage_info['index']} {doc_content[:200]}\")\n",
        "\n",
        "    document_formatted = \"\\n\".join(document_parts)\n",
        "\n",
        "    # 1차 프롬프트 템플릿: 가장 관련 깊은 문서 번호 선택 유도\n",
        "    prompt_first_turn = f\"\"\"상황: {query}\n",
        "{document_formatted}\n",
        "\n",
        "저 문서들(번호 1~{len(top_n_indices)}) 중 현 상황과 가장 관련이 깊은 문서의 **번호 번호(숫자)**만 답변해 줘.\"\"\"\n",
        "\n",
        "    return prompt_first_turn, top_n_indices, file_name\n",
        "\n",
        "def select_document_and_ask_sufficient_action(query: str, model_response_rank_raw: str, file_passages: List[Dict], top_n_indices: List[int], selected_doc_title: str) -> str:\n",
        "    \"\"\"\n",
        "    모델 응답에서 번호를 추출하여 문서를 선택하고, 조치의 충분성을 묻는 2차 프롬프트를 생성합니다.\n",
        "    \"\"\"\n",
        "    # 1. 모델 응답에서 번호 추출\n",
        "    response_content = model_response_rank_raw.split('</think>')[-1]\n",
        "    rank_match = re.search(r'\\d+', response_content)\n",
        "    selected_rank = int(rank_match.group(0)) if rank_match else 1\n",
        "\n",
        "    # 2. 선택된 문서 정보 가져오기 (유효성 검사 및 인덱싱)\n",
        "    if selected_rank > len(top_n_indices) or selected_rank <= 0:\n",
        "        print(f\"경고: 선택된 번호 번호({selected_rank})가 유효하지 않아 1위 문서로 대체합니다.\")\n",
        "        selected_rank = 1\n",
        "\n",
        "    try:\n",
        "        selected_doc_index = top_n_indices[selected_rank - 1]\n",
        "        selected_passage = file_passages[selected_doc_index]\n",
        "        selected_content_type = selected_passage['type']\n",
        "        selected_index = selected_passage['index']\n",
        "        selected_doc_content = selected_passage['content'].replace('\\n', ' ')\n",
        "    except IndexError:\n",
        "        # 매우 드문 케이스, 이미 위에서 처리했으나 안전을 위해 추가\n",
        "        print(\"치명적 오류: top_n_indices 인덱싱 실패. 첫 번째 문서로 대체.\")\n",
        "        selected_passage = file_passages[top_n_indices[0]]\n",
        "        selected_content_type = selected_passage['type']\n",
        "        selected_index = selected_passage['index']\n",
        "        selected_doc_content = selected_passage['content'].replace('\\n', ' ')\n",
        "\n",
        "    # 3. 2차 프롬프트 템플릿 정의 및 포맷: 조치 답변 생성\n",
        "    doc_identifier = f\"{selected_content_type} {selected_index}\"\n",
        "\n",
        "    prompt_second_turn = f\"\"\"\n",
        "상황: {query}\n",
        "\n",
        "--- 선택된 문서 ({selected_doc_title}, {doc_identifier}) ---\n",
        "{selected_doc_content}\n",
        "--------------------------------\n",
        "\n",
        "위 선택된 문서를 참고하여, 현 상황에 대해 **가장 적절한 조치**를 짧게 답변해 줘.\"\"\"\n",
        "\n",
        "    return prompt_second_turn\n",
        "\n",
        "def create_rewriting_prompt(full_response: str) -> str:\n",
        "    \"\"\"다음 검색에 최적화된 쿼리를 생성하는 프롬프트를 만듭니다.\"\"\"\n",
        "    rewriting_prompt = f\"\"\"\n",
        "다음은 이전 단계에서 '불충분'하다고 판단된 최종 조치 결과입니다.\n",
        "\n",
        "--- 이전 조치 답변 ---\n",
        "{full_response}\n",
        "--------------------\n",
        "\n",
        "이 답변의 내용을 기반으로, **현 상황을 해결하기 위해 다음 턴에 검색을 수행할 용도의 가장 핵심적이고 구체적인 질문 1개**만 생성해 주세요. 질문은 10단어 이내로 간결해야 합니다. (예: '압축기 교체 후 추가 점검 사항')\n",
        "\"\"\"\n",
        "    return rewriting_prompt"
      ],
      "metadata": {
        "id": "cozr15ilCkgm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 4. 초기 설정 및 순차적 RAG 루프 실행\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. LLM 초기화\n",
        "    initialize_llm(model_name)\n",
        "\n",
        "    if model is None:\n",
        "        exit() # 모델 로드 실패 시 종료\n",
        "\n",
        "    # 2. 문서 파싱 설정\n",
        "    documents_folder_path = \"/content\"\n",
        "    documents_list = parse_folder_documents(documents_folder_path)\n",
        "\n",
        "    if not documents_list:\n",
        "        print(\"FATAL ERROR: No documents loaded. Please ensure files exist at the path.\")\n",
        "        exit()\n",
        "\n",
        "    # 문서 순서 정의\n",
        "    DOCUMENT_ORDER = [\n",
        "        '문서_샘플_1.pdf',\n",
        "        '문서_샘플_2.pdf',\n",
        "        '문서_샘플_3.pdf',\n",
        "    ]\n",
        "    MAX_ITERATIONS = len(DOCUMENT_ORDER)\n",
        "\n",
        "    # 초기 쿼리 설정\n",
        "    initial_query = \"\"\"CH21 에러 발생으로 실내기 운전 불가 에러 최초발생시점 : 2025.06.03, 에러 최신발생시점 : 2025.07.08\\n누적발생일수 : 33, ch21 : 106, ch26: 0, ch29 :0, 백업inv1 : 정상, 백업inv2 : 정상\"\"\"\n",
        "    # initial_query = input()\n",
        "    current_query = initial_query\n",
        "    iteration_history = []\n",
        "\n",
        "    print(f\"\\n🚀 Initial Query: {initial_query}\")\n",
        "\n",
        "    # 3. 순차적 RAG 루프 실행\n",
        "    for i, doc_name_to_search in enumerate(DOCUMENT_ORDER):\n",
        "        print(\"\\n\" + \"#\"*80)\n",
        "        print(f\"🔄 ITERATION {i+1}/{MAX_ITERATIONS}\")\n",
        "        # print(f\"   현재 검색 쿼리: {current_query}\")\n",
        "        print(\"#\"*80)\n",
        "\n",
        "        # [문서 단위 처리] 문서 객체 찾기\n",
        "        document_unit: Dict[str, Any] = next((doc for doc in documents_list if doc['filename'] == doc_name_to_search), {})\n",
        "\n",
        "        if not document_unit or not document_unit.get('passages'):\n",
        "            print(f\"Warning: Document '{doc_name_to_search}' not found or has no content. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        file_passages: List[Dict] = document_unit['passages']\n",
        "\n",
        "        # A. 1차 단계: BM25 검색 및 문서 번호 선택 프롬프트 생성\n",
        "        prompt_to_get_rank, top_n_indices, selected_doc_title = retrieve_and_augment_by_file(\n",
        "            query=current_query,\n",
        "            file_passages=file_passages,\n",
        "            file_name=doc_name_to_search,\n",
        "            N=5\n",
        "        )\n",
        "\n",
        "        # B. 1차 모델 호출: 관련 문서 번호(숫자) 답변 받기\n",
        "        try:\n",
        "            rank_response_raw = call_llm_generate(prompt_to_get_rank, max_new_tokens=20)\n",
        "            rank_response = rank_response_raw.split('</think>')[-1].strip()\n",
        "        except RuntimeError as e:\n",
        "            print(f\"LLM Call Error: {e}\")\n",
        "            break\n",
        "\n",
        "        # print(f\"\\n💡 LLM 선택 번호: {rank_response}\")\n",
        "\n",
        "        # C. 2차 단계: 선택된 문서로 최종 조치 프롬프트 생성\n",
        "        prompt_to_get_action = select_document_and_ask_sufficient_action(\n",
        "            query=current_query,\n",
        "            model_response_rank_raw=rank_response_raw,\n",
        "            file_passages=file_passages,\n",
        "            top_n_indices=top_n_indices,\n",
        "            selected_doc_title=selected_doc_title\n",
        "        )\n",
        "\n",
        "        # D. 2차 모델 호출: 최종 조치 답변 받기\n",
        "        try:\n",
        "            action_response_raw = call_llm_generate(prompt_to_get_action, max_new_tokens=128)\n",
        "            action_response = action_response_raw.split('</think>')[-1].strip()\n",
        "        except RuntimeError as e:\n",
        "            print(f\"LLM Call Error: {e}\")\n",
        "            break\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"➡️ ITERATION {i+1} 답변: {action_response}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # E. 히스토리 저장 및 다음 턴 준비\n",
        "        iteration_history.append({\n",
        "            'turn': i + 1,\n",
        "            'doc_name': doc_name_to_search,\n",
        "            'query_for_search': current_query,\n",
        "            'action_response': action_response\n",
        "        })\n",
        "\n",
        "        is_last_iteration = (i + 1) == MAX_ITERATIONS\n",
        "\n",
        "        if is_last_iteration:\n",
        "            print(\"\\n✅ 모든 문서를 검색 완료했습니다. 반복을 종료합니다.\")\n",
        "            break\n",
        "\n",
        "        # 다음 턴을 위한 새로운 검색 쿼리 생성\n",
        "        rewriting_prompt = create_rewriting_prompt(action_response)\n",
        "        print(\"\\n🔍 새로운 검색 쿼리 생성을 위해 LLM 호출...\")\n",
        "        try:\n",
        "            # 새로운 쿼리 생성 (max_new_tokens=20)\n",
        "            new_query_response_raw = call_llm_generate(rewriting_prompt, max_new_tokens=20)\n",
        "            # 모델 응답에서 질문 부분만 깔끔하게 추출\n",
        "            current_query = new_query_response_raw.split(\"</think>\")[-1]\n",
        "        except RuntimeError as e:\n",
        "            print(f\"LLM Call Error during query rewriting: {e}\")\n",
        "            break\n",
        "\n",
        "        print(f\"새로운 검색 쿼리: {current_query}\")\n",
        "        current_query+=initial_query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tbRcBU8CnXW",
        "outputId": "d90b3f56-05c0-4b51-a520-963d83d80bea"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: LGAI-EXAONE/EXAONE-4.0-1.2B...\n",
            "Model and Tokenizer loaded successfully.\n",
            "\n",
            "🚀 Initial Query: CH21 에러 발생으로 실내기 운전 불가 에러 최초발생시점 : 2025.06.03, 에러 최신발생시점 : 2025.07.08\n",
            "누적발생일수 : 33, ch21 : 106, ch26: 0, ch29 :0, 백업inv1 : 정상, 백업inv2 : 정상\n",
            "\n",
            "################################################################################\n",
            "🔄 ITERATION 1/3\n",
            "################################################################################\n",
            "--------------------------------------------------\n",
            "🔎 BM25 검색 시작 (문서: 문서_샘플_1.pdf)\n",
            "--------------------------------------------------\n",
            "번호 1. 페이지 13 12 / 61 LGE Internal Use Only 1. 압축기점검이력이존재합니다. TMS AI 고장예측가이드북 압축기점검 ※ 압축기점검테이블포맷예) 해당시스템(실외기)의일단위실외기운전이력(모드, 최대주파수운전이력) 및이에매칭되는압축기주요에러발생이력에대한정보를제공. ①: 최초발생시점: - 일단위점검판정에해당하는압축기에러의최초발생시점 ②: 최신발생시점: -\n",
            "번호 2. 페이지 10 9 / 61 LGE Internal Use Only TMS AI 고장예측가이드북 압축기점검 ※ 압축기점검이란무엇인가요? 압축기점검 ①. 압축기에러: ch21, ch26, ch29의에러가일단위정상적인사용을저해하는수준으로지속발생하는경우를점검으로판정 ②. 백업이력체크: 데이터수집일최종일기준압축기가백업상태로유지된경우를점검으로판정 ③. 리포트(상세페이지)에서제공하는\n",
            "번호 3. 페이지 23 22 / 61 LGE Internal Use Only - 현장사례예) TMS AI 고장예측가이드북 FAN / 모터점검 ■REPAIR DESCRIPTION.(SE 점검이력) ■현장정보 ■그래프설명/점검로직검토결과 고객점검요구_구관4층실외기79에러팬,모터, 팬보드교체 1.    데이터분석기간: 21년5월1일– 21년10월31일(183일) 2.    해당에러최초\n",
            "번호 4. 페이지 22 21 / 61 LGE Internal Use Only TMS AI 고장예측가이드북 FAN / 모터점검 ① ② ③ ④ ※ Fan/모터점검그래프포맷예) ①: 최초발생시점 - 일단위점검판정에해당하는FAN2 오동작최초발생시점 ②: 최신발생시점 - 일단위수집된데이터범위내FAN2 오동작최종발생시점 ③: 누적발생일수 - 일단위점검판정에해당하는FAN2 오동작발생총누적일수\n",
            "번호 5. 페이지 14 13 / 61 LGE Internal Use Only - 현장사례예) TMS AI 고장예측가이드북 압축기점검 ■REPAIR DESCRIPTION.(SE 점검이력) ■현장정보 ■그래프설명/점검로직검토결과 2022-08-04  1:37:00 PM - 실외기압축기교체후냉매주입3kg. 2022-07-22  2:03:00 PM - 1번실외기29번으로, 압축기유상교체\n",
            "\n",
            "💡 LLM 선택 번호: **번호 1**\n",
            "\n",
            "================================================================================\n",
            "➡️ ITERATION 1 답변: **조치 답변:**  \n",
            "CH21 인버터압축기 IPM 고장 누적 106회 확인 → 시스템 장애 가능성 높음. 즉시 유지보수팀에 보고 및 Ch21 진단 수행 필요.\n",
            "================================================================================\n",
            "\n",
            "🔍 새로운 검색 쿼리 생성을 위해 LLM 호출...\n",
            "새로운 검색 쿼리: \n",
            "\n",
            "\"압축기 교체 후 추가 점검 사항은?\"\n",
            "\n",
            "################################################################################\n",
            "🔄 ITERATION 2/3\n",
            "################################################################################\n",
            "--------------------------------------------------\n",
            "🔎 BM25 검색 시작 (문서: 문서_샘플_2.pdf)\n",
            "--------------------------------------------------\n",
            "번호 1. 페이지 71 LGE Internal Use Only    3. 테이블 생성을 위한 데이터 항목  Webui_pReport_ODU_unit 테이블에 하기 값이 저장된다.  1) 에러 발생 정보  : comp_error_days   -  Unit_info == 0 이고, ID_Unit 이 '1~4'인 경우 NULL  - Unit_info == 0 이고, ID_Unit 이 \n",
            "번호 2. 페이지 12 LGE Internal Use Only  2 온라인 예방정비  2.1 압축기 점검  2.1.1 에러 이력  정의  압축기 에러 이력  Control Requirement ID  TMS_EHP_2_1_1  기능 요약  일 단위 압축기 에러 발생 이력을 요약한다  기능 진입 조건  데이터 유효성 검사를 통과한 데이터가 존재하는 경우에 로직에 돌입한다  기능 동\n",
            "번호 3. 페이지 70 LGE Internal Use Only  - 누적 발생일수(error_days): 센서 별 현상(점검/교체)의 개별 누적 발생일 수  - 최초 발생일(first_date): 센서 별 현상(점검/교체)의 최초 발생일  - 최신 발생일(last_date): 센서 별 현상(점검/교체)의 최종 발생일  - 센서 온도(chk_temp): 센서 별 현상(점검)의 감지\n",
            "번호 4. 페이지 72 LGE Internal Use Only  CH194_Cnt 합계를 개별 막대그래프로 표현한다.   2) 최대 RPM 이력    - 기준일로부터 6 개월간의 데이터를 일별로 집계한다    - 일일 단위의 Max_Fan_RPM 를 line plot 으로 Max_Fan_RPM_Err 를 scatter  plot 으로 표현한다.    - 에러 미발생 일자의 최대 \n",
            "번호 5. 페이지 14 LGE Internal Use Only  4. 수집된 데이터 기간 중, 에러가 감지된 최신일 기준: 직전 10 일 기간  내 에러  누적 10 회 이상인 경우 (and)       : A 그룹의 발생 횟수 ≥ B 그룹의 발생횟수 일 경우        ※ A 그룹, B 그룹의 정의            A 그룹: 발행일 기준 직전 5 일 이내(1 일이상부터 5\n",
            "\n",
            "💡 LLM 선택 번호: 1\n",
            "\n",
            "================================================================================\n",
            "➡️ ITERATION 2 답변: **조치 답변**:  \n",
            "CH21 에러 누적 106일차(2025.07.08) 발생 시, 해당 압축기(ID_Unit='V')의 CH21 점검 이력 확인 및 백업 시스템(BackupInv1) 재확인 필요. 최초 에러일(2025.06.03)부터 6개월 내 CH21_Cnt > 0인 일자 수 기록 및 외부팬 점검 데이터 추가 검증.\n",
            "================================================================================\n",
            "\n",
            "🔍 새로운 검색 쿼리 생성을 위해 LLM 호출...\n",
            "새로운 검색 쿼리: \n",
            "\n",
            "\"CH21 압축기 ID_Unit='V' 백업 시스템 BackupInv1 상태 확인\n",
            "\n",
            "################################################################################\n",
            "🔄 ITERATION 3/3\n",
            "################################################################################\n",
            "--------------------------------------------------\n",
            "🔎 BM25 검색 시작 (문서: 문서_샘플_3.pdf)\n",
            "--------------------------------------------------\n",
            "번호 1. 페이지 62 62_   서비스유지보수진단가이드 Error 분류 실외기 1. 압축기에러:  CH21, CH26, CH29 2. 비상에러: CH24, CH32, CH33, CH34, CH35, CH36, CH62, CH150, CH151, CH187, CH190, CH193 3. 센서및부품: CH41, CH42, CH43, CH44, CH45, CH46, CH47, CH6\n",
            "번호 2. 페이지 267 MULTI VTM _267 의미 냉방: 목표저압(증발온도확보)을기준으로동작 -. 목표저압보다현재저압이높은경우: 압축기Hz ↑ -. 목표저압보다현재저압이낮은경우: 압축기Hz ↓ 난방: 목표고압(응축온도확보)을기준으로동작 -. 목표고압보다현재고압이높은경우: 압축기Hz ↓ -. 목표고압보다현재고압이낮은경우: 압축기Hz ↑ 정상범위: 목표압력대비현재압력±(60kP\n",
            "번호 3. 페이지 36 MULTI VTM _36 고장진단/예지 고압과다상승이상(가변패스막힘감지) 실외기제어이상 의미 시스템의고압을측정하기위한센서의이상징후를진단(압축기토출측) 주요 현상 냉/난방운전중, 시스템의고압이높고, Main EEV1,2가특정수치에고정되어있는경우 고객불만 요소 냉방약 주요발생 요인 부품불량(냉방시: 가변패스막힘) 점검사항 1. 가변패스코일저항값및동작전압확인 2\n",
            "번호 4. 페이지 58 MULTI VTM _58 Error Level 별현상 제품에서사용되는에러는4개의레벨로구별되며, 레벨에따라시스템동작,  디스플레이, 해제방법이다르기때문에하기내용을참고하여정확히대응할수 있도록한다.  1. Error Level 1 1) 시스템동작: 지속정지 2) 디스플레이: 실내유선리모컨, 중앙제어기, LGMV, 실외기7-Segment 3) 해제방법: 전원을리셋\n",
            "번호 5. 페이지 63 63_  서비스유지보수진단가이드 Error 별대처방안 실외기– 압축기에러 CH 21 [인버터압축기IPM Fault] 의미 IPM자체보호회로동작(과전류/IPM과열/Vcc저전압) 주요 현상 IPM 내부에서이상과전류감지시, ‘Error Display’ 후제품정지 (발생시, Error Level 3 / 일정횟수이상발생시Error Level 1) 고객불만 요소 CH\n",
            "\n",
            "💡 LLM 선택 번호: 1\n",
            "\n",
            "================================================================================\n",
            "➡️ ITERATION 3 답변: **조치:** CH21 압축기 점검 및 교체\n",
            "================================================================================\n",
            "\n",
            "✅ 모든 문서를 검색 완료했습니다. 반복을 종료합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1mrWag4MF8NL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Curation"
      ],
      "metadata": {
        "id": "0CfJyM77HeOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
      ],
      "metadata": {
        "id": "qUrQUxgsDoed"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
