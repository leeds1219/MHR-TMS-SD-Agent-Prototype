{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "0CfJyM77HeOi"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPQMMW9N1viUjx8Cv4J2uGP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LG Pilot Demo"
      ],
      "metadata": {
        "id": "NK-5ziNEESfD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch PyMuPDF python-docx python-pptx konlpy rank-bm25"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_E3ovhuFkCE",
        "outputId": "c3efe02f-fb56-4dd0-abb3-ffb070b1bd4b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Collecting PyMuPDF\n",
            "  Downloading pymupdf-1.26.5-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting python-docx\n",
            "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting python-pptx\n",
            "  Downloading python_pptx-1.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
            "Collecting rank-bm25\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.35.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from python-docx) (5.4.0)\n",
            "Requirement already satisfied: Pillow>=3.3.2 in /usr/local/lib/python3.12/dist-packages (from python-pptx) (11.3.0)\n",
            "Collecting XlsxWriter>=0.5.7 (from python-pptx)\n",
            "  Downloading xlsxwriter-3.2.9-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting JPype1>=0.7.0 (from konlpy)\n",
            "  Downloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.10)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.10.5)\n",
            "Downloading pymupdf-1.26.5-cp39-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_pptx-1.0.2-py3-none-any.whl (472 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m472.8/472.8 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m124.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Downloading jpype1-1.6.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (495 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m495.9/495.9 kB\u001b[0m \u001b[31m42.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xlsxwriter-3.2.9-py3-none-any.whl (175 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m175.3/175.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: XlsxWriter, rank-bm25, python-docx, PyMuPDF, JPype1, python-pptx, konlpy\n",
            "Successfully installed JPype1-1.6.0 PyMuPDF-1.26.5 XlsxWriter-3.2.9 konlpy-0.6.0 python-docx-1.2.0 python-pptx-1.0.2 rank-bm25-0.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import re\n",
        "import os\n",
        "from typing import Dict, List, Any\n",
        "\n",
        "# PDF ì²˜ë¦¬\n",
        "import fitz\n",
        "# DOCX ì²˜ë¦¬\n",
        "from docx import Document\n",
        "# PPTX ì²˜ë¦¬\n",
        "from pptx import Presentation\n",
        "from konlpy.tag import Okt\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# ==============================================================================\n",
        "# 1. ëª¨ë¸/í† í¬ë‚˜ì´ì € ì´ˆê¸°í™” ë° í™˜ê²½ ì„¤ì •\n",
        "# ==============================================================================\n",
        "\n",
        "model_name = \"LGAI-EXAONE/EXAONE-4.0-1.2B\"\n",
        "model = None\n",
        "tokenizer = None\n",
        "okt = Okt() # KonLPy Okt ì´ˆê¸°í™”\n",
        "\n",
        "def initialize_llm(model_name: str):\n",
        "    \"\"\"LLM ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ê³  ì „ì—­ ë³€ìˆ˜ì— í• ë‹¹í•©ë‹ˆë‹¤.\"\"\"\n",
        "    global model, tokenizer\n",
        "    try:\n",
        "        print(f\"Loading model: {model_name}...\")\n",
        "        # â­ GPU í™˜ê²½ì—ì„œ ì‹¤í–‰ì„ ìœ„í•´ device_map=\"auto\"ì™€ dtype=\"bfloat16\" ì‚¬ìš©\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            dtype=torch.bfloat16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        print(\"Model and Tokenizer loaded successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading model: {e}\")\n",
        "        model, tokenizer = None, None\n",
        "        print(\"FATAL: Model loading failed. LLM generation will be impossible.\")\n",
        "\n",
        "def tokenize_ko_en(text: str) -> List[str]:\n",
        "    \"\"\"í•œê¸€ì€ í˜•íƒœì†Œ ë¶„ì„(ìŠ¤í…Œë° í¬í•¨), ì˜ì–´ëŠ” ì†Œë¬¸ìí™” ë° ê³µë°± ë¶„ë¦¬í•˜ì—¬ í† í° ë°˜í™˜\"\"\"\n",
        "    # í•œê¸€ í˜•íƒœì†Œ ë¶„ì„\n",
        "    ko_tokens = okt.morphs(text, stem=True)\n",
        "\n",
        "    # ì˜ì–´ ë° ê¸°íƒ€ í…ìŠ¤íŠ¸ ì²˜ë¦¬ (í˜•íƒœì†Œ ë¶„ì„ ê²°ê³¼ì— í¬í•¨ë˜ì§€ ì•Šì€ í† í°)\n",
        "    text_words = set(text.split())\n",
        "\n",
        "    en_tokens = [\n",
        "        token.lower()\n",
        "        for token in text_words\n",
        "        if token.lower() not in ko_tokens and token.isalnum() # ìˆœìˆ˜ ì˜ì–´ ë‹¨ì–´ ì¶”ì¶œ ì‹œ ìœ íš¨ì„± ê²€ì‚¬ ì¶”ê°€\n",
        "    ]\n",
        "    return ko_tokens + en_tokens"
      ],
      "metadata": {
        "id": "bGc81CFoZobM"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 2. ë¬¸ì„œ ì²˜ë¦¬ í•¨ìˆ˜\n",
        "# ==============================================================================\n",
        "\n",
        "def parse_pdf(file_path: str) -> List[Dict]:\n",
        "    \"\"\"PDF íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
        "    pages_data = []\n",
        "    try:\n",
        "        doc = fitz.open(file_path)\n",
        "        for page_num in range(doc.page_count):\n",
        "            page = doc.load_page(page_num)\n",
        "            text = page.get_text(\"text\")\n",
        "            if text.strip():\n",
        "                pages_data.append({'type': 'page', 'index': page_num + 1, 'content': text.strip()})\n",
        "        doc.close()\n",
        "    except Exception as e:\n",
        "        print(f\"PDF Parsing Error in {file_path}: {e}\")\n",
        "    return pages_data\n",
        "\n",
        "def parse_pptx(file_path: str) -> List[Dict]:\n",
        "    \"\"\"PPTX íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ ìŠ¬ë¼ì´ë“œë³„ í…ìŠ¤íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
        "    slides_data = []\n",
        "    try:\n",
        "        prs = Presentation(file_path)\n",
        "        for slide_num, slide in enumerate(prs.slides):\n",
        "            slide_text = []\n",
        "            for shape in slide.shapes:\n",
        "                if hasattr(shape, \"text\") and shape.has_text_frame:\n",
        "                    slide_text.append(shape.text)\n",
        "\n",
        "            content = \"\\n\".join(t.strip() for t in slide_text if t.strip())\n",
        "            if content:\n",
        "                 slides_data.append({'type': 'slide', 'index': slide_num + 1, 'content': content})\n",
        "    except Exception as e:\n",
        "        print(f\"PPTX Parsing Error in {file_path}: {e}\")\n",
        "    return slides_data\n",
        "\n",
        "def parse_docx(file_path: str) -> List[Dict]:\n",
        "    \"\"\"DOCX íŒŒì¼ì„ íŒŒì‹±í•˜ì—¬ ë‹¨ë½ë³„ í…ìŠ¤íŠ¸ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
        "    paragraphs_data = []\n",
        "    try:\n",
        "        doc = Document(file_path)\n",
        "        for para_num, paragraph in enumerate(doc.paragraphs):\n",
        "            text = paragraph.text.strip()\n",
        "            if text:\n",
        "                paragraphs_data.append({'type': 'paragraph', 'index': para_num + 1, 'content': text})\n",
        "    except Exception as e:\n",
        "        print(f\"DOCX Parsing Error in {file_path}: {e}\")\n",
        "    return paragraphs_data\n",
        "\n",
        "def parse_folder_documents(folder_path: str) -> List[Dict[str, Any]]:\n",
        "    \"\"\"í´ë” ë‚´ì˜ ì§€ì›ë˜ëŠ” ë¬¸ì„œ íŒŒì¼ë“¤ì„ íŒŒì‹±í•˜ì—¬ 'ë¬¸ì„œ ê°ì²´ ë¦¬ìŠ¤íŠ¸' í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\"\"\"\n",
        "    all_documents_list: List[Dict[str, Any]] = []\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"Error: Folder path does not exist: {folder_path}\")\n",
        "        return []\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        if os.path.isfile(file_path):\n",
        "            extension = filename.split('.')[-1].lower()\n",
        "            parsed_data: List[Dict] = []\n",
        "\n",
        "            try:\n",
        "                if extension == 'pdf':\n",
        "                    parsed_data = parse_pdf(file_path)\n",
        "                elif extension == 'pptx':\n",
        "                    parsed_data = parse_pptx(file_path)\n",
        "                elif extension == 'docx':\n",
        "                    parsed_data = parse_docx(file_path)\n",
        "\n",
        "                if parsed_data:\n",
        "                    # ë¬¸ì„œ ë‹¨ìœ„ ê°ì²´ë¡œ êµ¬ì„±\n",
        "                    all_documents_list.append({\n",
        "                        'filename': filename,\n",
        "                        'passages': parsed_data\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "    return all_documents_list"
      ],
      "metadata": {
        "id": "hr4RujmNWaGV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 3. RAG/BM25 ë° LLM ê´€ë ¨ í•¨ìˆ˜\n",
        "# ==============================================================================\n",
        "\n",
        "def call_llm_generate(prompt: str, max_new_tokens: int) -> str:\n",
        "    \"\"\"LLMì„ í˜¸ì¶œí•˜ê³  ì‘ë‹µ í…ìŠ¤íŠ¸ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤. ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.\"\"\"\n",
        "    global model, tokenizer\n",
        "    if model is None or tokenizer is None:\n",
        "        raise RuntimeError(\"FATAL ERROR: LLM Model or Tokenizer failed to load. Cannot perform generation.\")\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
        "\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=True,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(model.device) # ì…ë ¥ í…ì„œë¥¼ ëª¨ë¸ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response_raw = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    tag = \"</think>\"\n",
        "    if tag in response_raw:\n",
        "        return response_raw\n",
        "    else:\n",
        "        # ëª¨ë¸ì´ í”„ë¡¬í”„íŠ¸ë§Œ ë°˜í™˜í•˜ëŠ” ê²½ìš°, ì „ì²´ë¥¼ ë°˜í™˜\n",
        "        return response_raw\n",
        "\n",
        "def retrieve_and_augment_by_file(query: str, file_passages: List[Dict], file_name: str, N: int = 5):\n",
        "    \"\"\"\n",
        "    íŠ¹ì • íŒŒì¼ ë‚´ì˜ ì¡°ê°(Passages)ì— ëŒ€í•´ì„œë§Œ BM25 ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³  1ì°¨ í”„ë¡¬í”„íŠ¸ë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    if not file_passages:\n",
        "        return f\"ë¬¸ì„œ '{file_name}'ì—ëŠ” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.\", None, None\n",
        "\n",
        "    # ì½”í¼ìŠ¤ í† í°í™” ë° BM25 ì¸ë±ìŠ¤ ìƒì„±\n",
        "    tokenized_corpus = [tokenize_ko_en(p['content']) for p in file_passages]\n",
        "    bm25 = BM25Okapi(tokenized_corpus)\n",
        "\n",
        "    # ì§ˆë¬¸ í† í°í™” ë° ì ìˆ˜ ê³„ì‚°\n",
        "    tokenized_query = tokenize_ko_en(query)\n",
        "    doc_scores = bm25.get_scores(tokenized_query)\n",
        "\n",
        "    # ê´€ë ¨ì„± ë†’ì€ ìƒìœ„ Nê°œ ì¸ë±ìŠ¤ ì¶”ì¶œ\n",
        "    top_n_indices = doc_scores.argsort()[::-1][:N]\n",
        "\n",
        "    print(\"-\" * 50)\n",
        "    print(f\"ğŸ” BM25 ê²€ìƒ‰ ì‹œì‘ (ë¬¸ì„œ: {file_name})\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    # ë¬¸ì„œ ë‚´ìš© í¬ë§·íŒ…\n",
        "    document_parts = []\n",
        "\n",
        "    for rank_idx, doc_index in enumerate(top_n_indices):\n",
        "        passage_info = file_passages[doc_index]\n",
        "        doc_content = passage_info['content'].replace('\\n', ' ')\n",
        "        document_parts.append(f\"ë²ˆí˜¸ {rank_idx + 1}. : {doc_content}\")\n",
        "        # ë””ë²„ê¹…ìš© ì¶œë ¥ì€ ìƒëµí•˜ê³  í”„ë¡¬í”„íŠ¸ì— ì§‘ì¤‘\n",
        "        print(f\"ë²ˆí˜¸ {rank_idx + 1}. í˜ì´ì§€ {passage_info['index']} {doc_content[:200]}\")\n",
        "\n",
        "    document_formatted = \"\\n\".join(document_parts)\n",
        "\n",
        "    # 1ì°¨ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿: ê°€ì¥ ê´€ë ¨ ê¹Šì€ ë¬¸ì„œ ë²ˆí˜¸ ì„ íƒ ìœ ë„\n",
        "    prompt_first_turn = f\"\"\"ìƒí™©: {query}\n",
        "{document_formatted}\n",
        "\n",
        "ì € ë¬¸ì„œë“¤(ë²ˆí˜¸ 1~{len(top_n_indices)}) ì¤‘ í˜„ ìƒí™©ê³¼ ê°€ì¥ ê´€ë ¨ì´ ê¹Šì€ ë¬¸ì„œì˜ **ë²ˆí˜¸ ë²ˆí˜¸(ìˆ«ì)**ë§Œ ë‹µë³€í•´ ì¤˜.\"\"\"\n",
        "\n",
        "    return prompt_first_turn, top_n_indices, file_name\n",
        "\n",
        "def select_document_and_ask_sufficient_action(query: str, model_response_rank_raw: str, file_passages: List[Dict], top_n_indices: List[int], selected_doc_title: str) -> str:\n",
        "    \"\"\"\n",
        "    ëª¨ë¸ ì‘ë‹µì—ì„œ ë²ˆí˜¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ë¬¸ì„œë¥¼ ì„ íƒí•˜ê³ , ì¡°ì¹˜ì˜ ì¶©ë¶„ì„±ì„ ë¬»ëŠ” 2ì°¨ í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
        "    \"\"\"\n",
        "    # 1. ëª¨ë¸ ì‘ë‹µì—ì„œ ë²ˆí˜¸ ì¶”ì¶œ\n",
        "    response_content = model_response_rank_raw.split('</think>')[-1]\n",
        "    rank_match = re.search(r'\\d+', response_content)\n",
        "    selected_rank = int(rank_match.group(0)) if rank_match else 1\n",
        "\n",
        "    # 2. ì„ íƒëœ ë¬¸ì„œ ì •ë³´ ê°€ì ¸ì˜¤ê¸° (ìœ íš¨ì„± ê²€ì‚¬ ë° ì¸ë±ì‹±)\n",
        "    if selected_rank > len(top_n_indices) or selected_rank <= 0:\n",
        "        print(f\"ê²½ê³ : ì„ íƒëœ ë²ˆí˜¸ ë²ˆí˜¸({selected_rank})ê°€ ìœ íš¨í•˜ì§€ ì•Šì•„ 1ìœ„ ë¬¸ì„œë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.\")\n",
        "        selected_rank = 1\n",
        "\n",
        "    try:\n",
        "        selected_doc_index = top_n_indices[selected_rank - 1]\n",
        "        selected_passage = file_passages[selected_doc_index]\n",
        "        selected_content_type = selected_passage['type']\n",
        "        selected_index = selected_passage['index']\n",
        "        selected_doc_content = selected_passage['content'].replace('\\n', ' ')\n",
        "    except IndexError:\n",
        "        # ë§¤ìš° ë“œë¬¸ ì¼€ì´ìŠ¤, ì´ë¯¸ ìœ„ì—ì„œ ì²˜ë¦¬í–ˆìœ¼ë‚˜ ì•ˆì „ì„ ìœ„í•´ ì¶”ê°€\n",
        "        print(\"ì¹˜ëª…ì  ì˜¤ë¥˜: top_n_indices ì¸ë±ì‹± ì‹¤íŒ¨. ì²« ë²ˆì§¸ ë¬¸ì„œë¡œ ëŒ€ì²´.\")\n",
        "        selected_passage = file_passages[top_n_indices[0]]\n",
        "        selected_content_type = selected_passage['type']\n",
        "        selected_index = selected_passage['index']\n",
        "        selected_doc_content = selected_passage['content'].replace('\\n', ' ')\n",
        "\n",
        "    # 3. 2ì°¨ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜ ë° í¬ë§·: ì¡°ì¹˜ ë‹µë³€ ìƒì„±\n",
        "    doc_identifier = f\"{selected_content_type} {selected_index}\"\n",
        "\n",
        "    prompt_second_turn = f\"\"\"\n",
        "ìƒí™©: {query}\n",
        "\n",
        "--- ì„ íƒëœ ë¬¸ì„œ ({selected_doc_title}, {doc_identifier}) ---\n",
        "{selected_doc_content}\n",
        "--------------------------------\n",
        "\n",
        "ìœ„ ì„ íƒëœ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬, í˜„ ìƒí™©ì— ëŒ€í•´ **ê°€ì¥ ì ì ˆí•œ ì¡°ì¹˜**ë¥¼ ì§§ê²Œ ë‹µë³€í•´ ì¤˜.\"\"\"\n",
        "\n",
        "    return prompt_second_turn\n",
        "\n",
        "def create_rewriting_prompt(full_response: str) -> str:\n",
        "    \"\"\"ë‹¤ìŒ ê²€ìƒ‰ì— ìµœì í™”ëœ ì¿¼ë¦¬ë¥¼ ìƒì„±í•˜ëŠ” í”„ë¡¬í”„íŠ¸ë¥¼ ë§Œë“­ë‹ˆë‹¤.\"\"\"\n",
        "    rewriting_prompt = f\"\"\"\n",
        "ë‹¤ìŒì€ ì´ì „ ë‹¨ê³„ì—ì„œ 'ë¶ˆì¶©ë¶„'í•˜ë‹¤ê³  íŒë‹¨ëœ ìµœì¢… ì¡°ì¹˜ ê²°ê³¼ì…ë‹ˆë‹¤.\n",
        "\n",
        "--- ì´ì „ ì¡°ì¹˜ ë‹µë³€ ---\n",
        "{full_response}\n",
        "--------------------\n",
        "\n",
        "ì´ ë‹µë³€ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ, **í˜„ ìƒí™©ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ë‹¤ìŒ í„´ì— ê²€ìƒ‰ì„ ìˆ˜í–‰í•  ìš©ë„ì˜ ê°€ì¥ í•µì‹¬ì ì´ê³  êµ¬ì²´ì ì¸ ì§ˆë¬¸ 1ê°œ**ë§Œ ìƒì„±í•´ ì£¼ì„¸ìš”. ì§ˆë¬¸ì€ 10ë‹¨ì–´ ì´ë‚´ë¡œ ê°„ê²°í•´ì•¼ í•©ë‹ˆë‹¤. (ì˜ˆ: 'ì••ì¶•ê¸° êµì²´ í›„ ì¶”ê°€ ì ê²€ ì‚¬í•­')\n",
        "\"\"\"\n",
        "    return rewriting_prompt"
      ],
      "metadata": {
        "id": "cozr15ilCkgm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ==============================================================================\n",
        "# 4. ì´ˆê¸° ì„¤ì • ë° ìˆœì°¨ì  RAG ë£¨í”„ ì‹¤í–‰\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. LLM ì´ˆê¸°í™”\n",
        "    initialize_llm(model_name)\n",
        "\n",
        "    if model is None:\n",
        "        exit() # ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ì‹œ ì¢…ë£Œ\n",
        "\n",
        "    # 2. ë¬¸ì„œ íŒŒì‹± ì„¤ì •\n",
        "    documents_folder_path = \"/content\"\n",
        "    documents_list = parse_folder_documents(documents_folder_path)\n",
        "\n",
        "    if not documents_list:\n",
        "        print(\"FATAL ERROR: No documents loaded. Please ensure files exist at the path.\")\n",
        "        exit()\n",
        "\n",
        "    # ë¬¸ì„œ ìˆœì„œ ì •ì˜\n",
        "    DOCUMENT_ORDER = [\n",
        "        'ë¬¸ì„œ_ìƒ˜í”Œ_1.pdf',\n",
        "        'ë¬¸ì„œ_ìƒ˜í”Œ_2.pdf',\n",
        "        'ë¬¸ì„œ_ìƒ˜í”Œ_3.pdf',\n",
        "    ]\n",
        "    MAX_ITERATIONS = len(DOCUMENT_ORDER)\n",
        "\n",
        "    # ì´ˆê¸° ì¿¼ë¦¬ ì„¤ì •\n",
        "    initial_query = \"\"\"CH21 ì—ëŸ¬ ë°œìƒìœ¼ë¡œ ì‹¤ë‚´ê¸° ìš´ì „ ë¶ˆê°€ ì—ëŸ¬ ìµœì´ˆë°œìƒì‹œì  : 2025.06.03, ì—ëŸ¬ ìµœì‹ ë°œìƒì‹œì  : 2025.07.08\\nëˆ„ì ë°œìƒì¼ìˆ˜ : 33, ch21 : 106, ch26: 0, ch29 :0, ë°±ì—…inv1 : ì •ìƒ, ë°±ì—…inv2 : ì •ìƒ\"\"\"\n",
        "    # initial_query = input()\n",
        "    current_query = initial_query\n",
        "    iteration_history = []\n",
        "\n",
        "    print(f\"\\nğŸš€ Initial Query: {initial_query}\")\n",
        "\n",
        "    # 3. ìˆœì°¨ì  RAG ë£¨í”„ ì‹¤í–‰\n",
        "    for i, doc_name_to_search in enumerate(DOCUMENT_ORDER):\n",
        "        print(\"\\n\" + \"#\"*80)\n",
        "        print(f\"ğŸ”„ ITERATION {i+1}/{MAX_ITERATIONS}\")\n",
        "        # print(f\" Â  í˜„ì¬ ê²€ìƒ‰ ì¿¼ë¦¬: {current_query}\")\n",
        "        print(\"#\"*80)\n",
        "\n",
        "        # [ë¬¸ì„œ ë‹¨ìœ„ ì²˜ë¦¬] ë¬¸ì„œ ê°ì²´ ì°¾ê¸°\n",
        "        document_unit: Dict[str, Any] = next((doc for doc in documents_list if doc['filename'] == doc_name_to_search), {})\n",
        "\n",
        "        if not document_unit or not document_unit.get('passages'):\n",
        "            print(f\"Warning: Document '{doc_name_to_search}' not found or has no content. Skipping.\")\n",
        "            continue\n",
        "\n",
        "        file_passages: List[Dict] = document_unit['passages']\n",
        "\n",
        "        # A. 1ì°¨ ë‹¨ê³„: BM25 ê²€ìƒ‰ ë° ë¬¸ì„œ ë²ˆí˜¸ ì„ íƒ í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
        "        prompt_to_get_rank, top_n_indices, selected_doc_title = retrieve_and_augment_by_file(\n",
        "            query=current_query,\n",
        "            file_passages=file_passages,\n",
        "            file_name=doc_name_to_search,\n",
        "            N=5\n",
        "        )\n",
        "\n",
        "        # B. 1ì°¨ ëª¨ë¸ í˜¸ì¶œ: ê´€ë ¨ ë¬¸ì„œ ë²ˆí˜¸(ìˆ«ì) ë‹µë³€ ë°›ê¸°\n",
        "        try:\n",
        "            rank_response_raw = call_llm_generate(prompt_to_get_rank, max_new_tokens=20)\n",
        "            rank_response = rank_response_raw.split('</think>')[-1].strip()\n",
        "        except RuntimeError as e:\n",
        "            print(f\"LLM Call Error: {e}\")\n",
        "            break\n",
        "\n",
        "        # print(f\"\\nğŸ’¡ LLM ì„ íƒ ë²ˆí˜¸: {rank_response}\")\n",
        "\n",
        "        # C. 2ì°¨ ë‹¨ê³„: ì„ íƒëœ ë¬¸ì„œë¡œ ìµœì¢… ì¡°ì¹˜ í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
        "        prompt_to_get_action = select_document_and_ask_sufficient_action(\n",
        "            query=current_query,\n",
        "            model_response_rank_raw=rank_response_raw,\n",
        "            file_passages=file_passages,\n",
        "            top_n_indices=top_n_indices,\n",
        "            selected_doc_title=selected_doc_title\n",
        "        )\n",
        "\n",
        "        # D. 2ì°¨ ëª¨ë¸ í˜¸ì¶œ: ìµœì¢… ì¡°ì¹˜ ë‹µë³€ ë°›ê¸°\n",
        "        try:\n",
        "            action_response_raw = call_llm_generate(prompt_to_get_action, max_new_tokens=128)\n",
        "            action_response = action_response_raw.split('</think>')[-1].strip()\n",
        "        except RuntimeError as e:\n",
        "            print(f\"LLM Call Error: {e}\")\n",
        "            break\n",
        "\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(f\"â¡ï¸ ITERATION {i+1} ë‹µë³€: {action_response}\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        # E. íˆìŠ¤í† ë¦¬ ì €ì¥ ë° ë‹¤ìŒ í„´ ì¤€ë¹„\n",
        "        iteration_history.append({\n",
        "            'turn': i + 1,\n",
        "            'doc_name': doc_name_to_search,\n",
        "            'query_for_search': current_query,\n",
        "            'action_response': action_response\n",
        "        })\n",
        "\n",
        "        is_last_iteration = (i + 1) == MAX_ITERATIONS\n",
        "\n",
        "        if is_last_iteration:\n",
        "            print(\"\\nâœ… ëª¨ë“  ë¬¸ì„œë¥¼ ê²€ìƒ‰ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ë°˜ë³µì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\")\n",
        "            break\n",
        "\n",
        "        # ë‹¤ìŒ í„´ì„ ìœ„í•œ ìƒˆë¡œìš´ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±\n",
        "        rewriting_prompt = create_rewriting_prompt(action_response)\n",
        "        print(\"\\nğŸ” ìƒˆë¡œìš´ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±ì„ ìœ„í•´ LLM í˜¸ì¶œ...\")\n",
        "        try:\n",
        "            # ìƒˆë¡œìš´ ì¿¼ë¦¬ ìƒì„± (max_new_tokens=20)\n",
        "            new_query_response_raw = call_llm_generate(rewriting_prompt, max_new_tokens=20)\n",
        "            # ëª¨ë¸ ì‘ë‹µì—ì„œ ì§ˆë¬¸ ë¶€ë¶„ë§Œ ê¹”ë”í•˜ê²Œ ì¶”ì¶œ\n",
        "            current_query = new_query_response_raw.split(\"</think>\")[-1]\n",
        "        except RuntimeError as e:\n",
        "            print(f\"LLM Call Error during query rewriting: {e}\")\n",
        "            break\n",
        "\n",
        "        print(f\"ìƒˆë¡œìš´ ê²€ìƒ‰ ì¿¼ë¦¬: {current_query}\")\n",
        "        current_query+=initial_query"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_tbRcBU8CnXW",
        "outputId": "d90b3f56-05c0-4b51-a520-963d83d80bea"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading model: LGAI-EXAONE/EXAONE-4.0-1.2B...\n",
            "Model and Tokenizer loaded successfully.\n",
            "\n",
            "ğŸš€ Initial Query: CH21 ì—ëŸ¬ ë°œìƒìœ¼ë¡œ ì‹¤ë‚´ê¸° ìš´ì „ ë¶ˆê°€ ì—ëŸ¬ ìµœì´ˆë°œìƒì‹œì  : 2025.06.03, ì—ëŸ¬ ìµœì‹ ë°œìƒì‹œì  : 2025.07.08\n",
            "ëˆ„ì ë°œìƒì¼ìˆ˜ : 33, ch21 : 106, ch26: 0, ch29 :0, ë°±ì—…inv1 : ì •ìƒ, ë°±ì—…inv2 : ì •ìƒ\n",
            "\n",
            "################################################################################\n",
            "ğŸ”„ ITERATION 1/3\n",
            "################################################################################\n",
            "--------------------------------------------------\n",
            "ğŸ” BM25 ê²€ìƒ‰ ì‹œì‘ (ë¬¸ì„œ: ë¬¸ì„œ_ìƒ˜í”Œ_1.pdf)\n",
            "--------------------------------------------------\n",
            "ë²ˆí˜¸ 1. í˜ì´ì§€ 13 12 / 61 LGE Internal Use Only 1. ì••ì¶•ê¸°ì ê²€ì´ë ¥ì´ì¡´ì¬í•©ë‹ˆë‹¤. TMS AI ê³ ì¥ì˜ˆì¸¡ê°€ì´ë“œë¶ ì••ì¶•ê¸°ì ê²€ â€» ì••ì¶•ê¸°ì ê²€í…Œì´ë¸”í¬ë§·ì˜ˆ) í•´ë‹¹ì‹œìŠ¤í…œ(ì‹¤ì™¸ê¸°)ì˜ì¼ë‹¨ìœ„ì‹¤ì™¸ê¸°ìš´ì „ì´ë ¥(ëª¨ë“œ, ìµœëŒ€ì£¼íŒŒìˆ˜ìš´ì „ì´ë ¥) ë°ì´ì—ë§¤ì¹­ë˜ëŠ”ì••ì¶•ê¸°ì£¼ìš”ì—ëŸ¬ë°œìƒì´ë ¥ì—ëŒ€í•œì •ë³´ë¥¼ì œê³µ. â‘ : ìµœì´ˆë°œìƒì‹œì : - ì¼ë‹¨ìœ„ì ê²€íŒì •ì—í•´ë‹¹í•˜ëŠ”ì••ì¶•ê¸°ì—ëŸ¬ì˜ìµœì´ˆë°œìƒì‹œì  â‘¡: ìµœì‹ ë°œìƒì‹œì : -\n",
            "ë²ˆí˜¸ 2. í˜ì´ì§€ 10 9 / 61 LGE Internal Use Only TMS AI ê³ ì¥ì˜ˆì¸¡ê°€ì´ë“œë¶ ì••ì¶•ê¸°ì ê²€ â€» ì••ì¶•ê¸°ì ê²€ì´ë€ë¬´ì—‡ì¸ê°€ìš”? ì••ì¶•ê¸°ì ê²€ â‘ . ì••ì¶•ê¸°ì—ëŸ¬: ch21, ch26, ch29ì˜ì—ëŸ¬ê°€ì¼ë‹¨ìœ„ì •ìƒì ì¸ì‚¬ìš©ì„ì €í•´í•˜ëŠ”ìˆ˜ì¤€ìœ¼ë¡œì§€ì†ë°œìƒí•˜ëŠ”ê²½ìš°ë¥¼ì ê²€ìœ¼ë¡œíŒì • â‘¡. ë°±ì—…ì´ë ¥ì²´í¬: ë°ì´í„°ìˆ˜ì§‘ì¼ìµœì¢…ì¼ê¸°ì¤€ì••ì¶•ê¸°ê°€ë°±ì—…ìƒíƒœë¡œìœ ì§€ëœê²½ìš°ë¥¼ì ê²€ìœ¼ë¡œíŒì • â‘¢. ë¦¬í¬íŠ¸(ìƒì„¸í˜ì´ì§€)ì—ì„œì œê³µí•˜ëŠ”\n",
            "ë²ˆí˜¸ 3. í˜ì´ì§€ 23 22 / 61 LGE Internal Use Only - í˜„ì¥ì‚¬ë¡€ì˜ˆ) TMS AI ê³ ì¥ì˜ˆì¸¡ê°€ì´ë“œë¶ FAN / ëª¨í„°ì ê²€ â– REPAIR DESCRIPTION.(SE ì ê²€ì´ë ¥) â– í˜„ì¥ì •ë³´ â– ê·¸ë˜í”„ì„¤ëª…/ì ê²€ë¡œì§ê²€í† ê²°ê³¼ ê³ ê°ì ê²€ìš”êµ¬_êµ¬ê´€4ì¸µì‹¤ì™¸ê¸°79ì—ëŸ¬íŒ¬,ëª¨í„°, íŒ¬ë³´ë“œêµì²´ 1.    ë°ì´í„°ë¶„ì„ê¸°ê°„: 21ë…„5ì›”1ì¼â€“ 21ë…„10ì›”31ì¼(183ì¼) 2.    í•´ë‹¹ì—ëŸ¬ìµœì´ˆ\n",
            "ë²ˆí˜¸ 4. í˜ì´ì§€ 22 21 / 61 LGE Internal Use Only TMS AI ê³ ì¥ì˜ˆì¸¡ê°€ì´ë“œë¶ FAN / ëª¨í„°ì ê²€ â‘  â‘¡ â‘¢ â‘£ â€» Fan/ëª¨í„°ì ê²€ê·¸ë˜í”„í¬ë§·ì˜ˆ) â‘ : ìµœì´ˆë°œìƒì‹œì  - ì¼ë‹¨ìœ„ì ê²€íŒì •ì—í•´ë‹¹í•˜ëŠ”FAN2 ì˜¤ë™ì‘ìµœì´ˆë°œìƒì‹œì  â‘¡: ìµœì‹ ë°œìƒì‹œì  - ì¼ë‹¨ìœ„ìˆ˜ì§‘ëœë°ì´í„°ë²”ìœ„ë‚´FAN2 ì˜¤ë™ì‘ìµœì¢…ë°œìƒì‹œì  â‘¢: ëˆ„ì ë°œìƒì¼ìˆ˜ - ì¼ë‹¨ìœ„ì ê²€íŒì •ì—í•´ë‹¹í•˜ëŠ”FAN2 ì˜¤ë™ì‘ë°œìƒì´ëˆ„ì ì¼ìˆ˜\n",
            "ë²ˆí˜¸ 5. í˜ì´ì§€ 14 13 / 61 LGE Internal Use Only - í˜„ì¥ì‚¬ë¡€ì˜ˆ) TMS AI ê³ ì¥ì˜ˆì¸¡ê°€ì´ë“œë¶ ì••ì¶•ê¸°ì ê²€ â– REPAIR DESCRIPTION.(SE ì ê²€ì´ë ¥) â– í˜„ì¥ì •ë³´ â– ê·¸ë˜í”„ì„¤ëª…/ì ê²€ë¡œì§ê²€í† ê²°ê³¼ 2022-08-04  1:37:00 PM - ì‹¤ì™¸ê¸°ì••ì¶•ê¸°êµì²´í›„ëƒ‰ë§¤ì£¼ì…3kg. 2022-07-22  2:03:00 PM - 1ë²ˆì‹¤ì™¸ê¸°29ë²ˆìœ¼ë¡œ, ì••ì¶•ê¸°ìœ ìƒêµì²´\n",
            "\n",
            "ğŸ’¡ LLM ì„ íƒ ë²ˆí˜¸: **ë²ˆí˜¸ 1**\n",
            "\n",
            "================================================================================\n",
            "â¡ï¸ ITERATION 1 ë‹µë³€: **ì¡°ì¹˜ ë‹µë³€:**  \n",
            "CH21 ì¸ë²„í„°ì••ì¶•ê¸° IPM ê³ ì¥ ëˆ„ì  106íšŒ í™•ì¸ â†’ ì‹œìŠ¤í…œ ì¥ì•  ê°€ëŠ¥ì„± ë†’ìŒ. ì¦‰ì‹œ ìœ ì§€ë³´ìˆ˜íŒ€ì— ë³´ê³  ë° Ch21 ì§„ë‹¨ ìˆ˜í–‰ í•„ìš”.\n",
            "================================================================================\n",
            "\n",
            "ğŸ” ìƒˆë¡œìš´ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±ì„ ìœ„í•´ LLM í˜¸ì¶œ...\n",
            "ìƒˆë¡œìš´ ê²€ìƒ‰ ì¿¼ë¦¬: \n",
            "\n",
            "\"ì••ì¶•ê¸° êµì²´ í›„ ì¶”ê°€ ì ê²€ ì‚¬í•­ì€?\"\n",
            "\n",
            "################################################################################\n",
            "ğŸ”„ ITERATION 2/3\n",
            "################################################################################\n",
            "--------------------------------------------------\n",
            "ğŸ” BM25 ê²€ìƒ‰ ì‹œì‘ (ë¬¸ì„œ: ë¬¸ì„œ_ìƒ˜í”Œ_2.pdf)\n",
            "--------------------------------------------------\n",
            "ë²ˆí˜¸ 1. í˜ì´ì§€ 71 LGE Internal Use Only    3. í…Œì´ë¸” ìƒì„±ì„ ìœ„í•œ ë°ì´í„° í•­ëª©  Webui_pReport_ODU_unit í…Œì´ë¸”ì— í•˜ê¸° ê°’ì´ ì €ì¥ëœë‹¤.  1) ì—ëŸ¬ ë°œìƒ ì •ë³´  : comp_error_days   -  Unit_info == 0 ì´ê³ , ID_Unit ì´ '1~4'ì¸ ê²½ìš° NULL  - Unit_info == 0 ì´ê³ , ID_Unit ì´ \n",
            "ë²ˆí˜¸ 2. í˜ì´ì§€ 12 LGE Internal Use Only  2 ì˜¨ë¼ì¸ ì˜ˆë°©ì •ë¹„  2.1 ì••ì¶•ê¸° ì ê²€  2.1.1 ì—ëŸ¬ ì´ë ¥  ì •ì˜  ì••ì¶•ê¸° ì—ëŸ¬ ì´ë ¥  Control Requirement ID  TMS_EHP_2_1_1  ê¸°ëŠ¥ ìš”ì•½  ì¼ ë‹¨ìœ„ ì••ì¶•ê¸° ì—ëŸ¬ ë°œìƒ ì´ë ¥ì„ ìš”ì•½í•œë‹¤  ê¸°ëŠ¥ ì§„ì… ì¡°ê±´  ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ë¥¼ í†µê³¼í•œ ë°ì´í„°ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°ì— ë¡œì§ì— ëŒì…í•œë‹¤  ê¸°ëŠ¥ ë™\n",
            "ë²ˆí˜¸ 3. í˜ì´ì§€ 70 LGE Internal Use Only  - ëˆ„ì  ë°œìƒì¼ìˆ˜(error_days): ì„¼ì„œ ë³„ í˜„ìƒ(ì ê²€/êµì²´)ì˜ ê°œë³„ ëˆ„ì  ë°œìƒì¼ ìˆ˜  - ìµœì´ˆ ë°œìƒì¼(first_date): ì„¼ì„œ ë³„ í˜„ìƒ(ì ê²€/êµì²´)ì˜ ìµœì´ˆ ë°œìƒì¼  - ìµœì‹  ë°œìƒì¼(last_date): ì„¼ì„œ ë³„ í˜„ìƒ(ì ê²€/êµì²´)ì˜ ìµœì¢… ë°œìƒì¼  - ì„¼ì„œ ì˜¨ë„(chk_temp): ì„¼ì„œ ë³„ í˜„ìƒ(ì ê²€)ì˜ ê°ì§€\n",
            "ë²ˆí˜¸ 4. í˜ì´ì§€ 72 LGE Internal Use Only  CH194_Cnt í•©ê³„ë¥¼ ê°œë³„ ë§‰ëŒ€ê·¸ë˜í”„ë¡œ í‘œí˜„í•œë‹¤.   2) ìµœëŒ€ RPM ì´ë ¥    - ê¸°ì¤€ì¼ë¡œë¶€í„° 6 ê°œì›”ê°„ì˜ ë°ì´í„°ë¥¼ ì¼ë³„ë¡œ ì§‘ê³„í•œë‹¤    - ì¼ì¼ ë‹¨ìœ„ì˜ Max_Fan_RPM ë¥¼ line plot ìœ¼ë¡œ Max_Fan_RPM_Err ë¥¼ scatter  plot ìœ¼ë¡œ í‘œí˜„í•œë‹¤.    - ì—ëŸ¬ ë¯¸ë°œìƒ ì¼ìì˜ ìµœëŒ€ \n",
            "ë²ˆí˜¸ 5. í˜ì´ì§€ 14 LGE Internal Use Only  4. ìˆ˜ì§‘ëœ ë°ì´í„° ê¸°ê°„ ì¤‘, ì—ëŸ¬ê°€ ê°ì§€ëœ ìµœì‹ ì¼ ê¸°ì¤€: ì§ì „ 10 ì¼ ê¸°ê°„  ë‚´ ì—ëŸ¬  ëˆ„ì  10 íšŒ ì´ìƒì¸ ê²½ìš° (and)       : A ê·¸ë£¹ì˜ ë°œìƒ íšŸìˆ˜ â‰¥ B ê·¸ë£¹ì˜ ë°œìƒíšŸìˆ˜ ì¼ ê²½ìš°        â€» A ê·¸ë£¹, B ê·¸ë£¹ì˜ ì •ì˜            A ê·¸ë£¹: ë°œí–‰ì¼ ê¸°ì¤€ ì§ì „ 5 ì¼ ì´ë‚´(1 ì¼ì´ìƒë¶€í„° 5\n",
            "\n",
            "ğŸ’¡ LLM ì„ íƒ ë²ˆí˜¸: 1\n",
            "\n",
            "================================================================================\n",
            "â¡ï¸ ITERATION 2 ë‹µë³€: **ì¡°ì¹˜ ë‹µë³€**:  \n",
            "CH21 ì—ëŸ¬ ëˆ„ì  106ì¼ì°¨(2025.07.08) ë°œìƒ ì‹œ, í•´ë‹¹ ì••ì¶•ê¸°(ID_Unit='V')ì˜ CH21 ì ê²€ ì´ë ¥ í™•ì¸ ë° ë°±ì—… ì‹œìŠ¤í…œ(BackupInv1) ì¬í™•ì¸ í•„ìš”. ìµœì´ˆ ì—ëŸ¬ì¼(2025.06.03)ë¶€í„° 6ê°œì›” ë‚´ CH21_Cnt > 0ì¸ ì¼ì ìˆ˜ ê¸°ë¡ ë° ì™¸ë¶€íŒ¬ ì ê²€ ë°ì´í„° ì¶”ê°€ ê²€ì¦.\n",
            "================================================================================\n",
            "\n",
            "ğŸ” ìƒˆë¡œìš´ ê²€ìƒ‰ ì¿¼ë¦¬ ìƒì„±ì„ ìœ„í•´ LLM í˜¸ì¶œ...\n",
            "ìƒˆë¡œìš´ ê²€ìƒ‰ ì¿¼ë¦¬: \n",
            "\n",
            "\"CH21 ì••ì¶•ê¸° ID_Unit='V' ë°±ì—… ì‹œìŠ¤í…œ BackupInv1 ìƒíƒœ í™•ì¸\n",
            "\n",
            "################################################################################\n",
            "ğŸ”„ ITERATION 3/3\n",
            "################################################################################\n",
            "--------------------------------------------------\n",
            "ğŸ” BM25 ê²€ìƒ‰ ì‹œì‘ (ë¬¸ì„œ: ë¬¸ì„œ_ìƒ˜í”Œ_3.pdf)\n",
            "--------------------------------------------------\n",
            "ë²ˆí˜¸ 1. í˜ì´ì§€ 62 62_   ì„œë¹„ìŠ¤ìœ ì§€ë³´ìˆ˜ì§„ë‹¨ê°€ì´ë“œ Error ë¶„ë¥˜ ì‹¤ì™¸ê¸° 1. ì••ì¶•ê¸°ì—ëŸ¬:  CH21, CH26, CH29 2. ë¹„ìƒì—ëŸ¬: CH24, CH32, CH33, CH34, CH35, CH36, CH62, CH150, CH151, CH187, CH190, CH193 3. ì„¼ì„œë°ë¶€í’ˆ: CH41, CH42, CH43, CH44, CH45, CH46, CH47, CH6\n",
            "ë²ˆí˜¸ 2. í˜ì´ì§€ 267 MULTI VTM _267 ì˜ë¯¸ ëƒ‰ë°©: ëª©í‘œì €ì••(ì¦ë°œì˜¨ë„í™•ë³´)ì„ê¸°ì¤€ìœ¼ë¡œë™ì‘ -. ëª©í‘œì €ì••ë³´ë‹¤í˜„ì¬ì €ì••ì´ë†’ì€ê²½ìš°: ì••ì¶•ê¸°Hz â†‘ -. ëª©í‘œì €ì••ë³´ë‹¤í˜„ì¬ì €ì••ì´ë‚®ì€ê²½ìš°: ì••ì¶•ê¸°Hz â†“ ë‚œë°©: ëª©í‘œê³ ì••(ì‘ì¶•ì˜¨ë„í™•ë³´)ì„ê¸°ì¤€ìœ¼ë¡œë™ì‘ -. ëª©í‘œê³ ì••ë³´ë‹¤í˜„ì¬ê³ ì••ì´ë†’ì€ê²½ìš°: ì••ì¶•ê¸°Hz â†“ -. ëª©í‘œê³ ì••ë³´ë‹¤í˜„ì¬ê³ ì••ì´ë‚®ì€ê²½ìš°: ì••ì¶•ê¸°Hz â†‘ ì •ìƒë²”ìœ„: ëª©í‘œì••ë ¥ëŒ€ë¹„í˜„ì¬ì••ë ¥Â±(60kP\n",
            "ë²ˆí˜¸ 3. í˜ì´ì§€ 36 MULTI VTM _36 ê³ ì¥ì§„ë‹¨/ì˜ˆì§€ ê³ ì••ê³¼ë‹¤ìƒìŠ¹ì´ìƒ(ê°€ë³€íŒ¨ìŠ¤ë§‰í˜ê°ì§€) ì‹¤ì™¸ê¸°ì œì–´ì´ìƒ ì˜ë¯¸ ì‹œìŠ¤í…œì˜ê³ ì••ì„ì¸¡ì •í•˜ê¸°ìœ„í•œì„¼ì„œì˜ì´ìƒì§•í›„ë¥¼ì§„ë‹¨(ì••ì¶•ê¸°í† ì¶œì¸¡) ì£¼ìš” í˜„ìƒ ëƒ‰/ë‚œë°©ìš´ì „ì¤‘, ì‹œìŠ¤í…œì˜ê³ ì••ì´ë†’ê³ , Main EEV1,2ê°€íŠ¹ì •ìˆ˜ì¹˜ì—ê³ ì •ë˜ì–´ìˆëŠ”ê²½ìš° ê³ ê°ë¶ˆë§Œ ìš”ì†Œ ëƒ‰ë°©ì•½ ì£¼ìš”ë°œìƒ ìš”ì¸ ë¶€í’ˆë¶ˆëŸ‰(ëƒ‰ë°©ì‹œ: ê°€ë³€íŒ¨ìŠ¤ë§‰í˜) ì ê²€ì‚¬í•­ 1. ê°€ë³€íŒ¨ìŠ¤ì½”ì¼ì €í•­ê°’ë°ë™ì‘ì „ì••í™•ì¸ 2\n",
            "ë²ˆí˜¸ 4. í˜ì´ì§€ 58 MULTI VTM _58 Error Level ë³„í˜„ìƒ ì œí’ˆì—ì„œì‚¬ìš©ë˜ëŠ”ì—ëŸ¬ëŠ”4ê°œì˜ë ˆë²¨ë¡œêµ¬ë³„ë˜ë©°, ë ˆë²¨ì—ë”°ë¼ì‹œìŠ¤í…œë™ì‘,  ë””ìŠ¤í”Œë ˆì´, í•´ì œë°©ë²•ì´ë‹¤ë¥´ê¸°ë•Œë¬¸ì—í•˜ê¸°ë‚´ìš©ì„ì°¸ê³ í•˜ì—¬ì •í™•íˆëŒ€ì‘í• ìˆ˜ ìˆë„ë¡í•œë‹¤.  1. Error Level 1 1) ì‹œìŠ¤í…œë™ì‘: ì§€ì†ì •ì§€ 2) ë””ìŠ¤í”Œë ˆì´: ì‹¤ë‚´ìœ ì„ ë¦¬ëª¨ì»¨, ì¤‘ì•™ì œì–´ê¸°, LGMV, ì‹¤ì™¸ê¸°7-Segment 3) í•´ì œë°©ë²•: ì „ì›ì„ë¦¬ì…‹\n",
            "ë²ˆí˜¸ 5. í˜ì´ì§€ 63 63_  ì„œë¹„ìŠ¤ìœ ì§€ë³´ìˆ˜ì§„ë‹¨ê°€ì´ë“œ Error ë³„ëŒ€ì²˜ë°©ì•ˆ ì‹¤ì™¸ê¸°â€“ ì••ì¶•ê¸°ì—ëŸ¬ CH 21 [ì¸ë²„í„°ì••ì¶•ê¸°IPM Fault] ì˜ë¯¸ IPMìì²´ë³´í˜¸íšŒë¡œë™ì‘(ê³¼ì „ë¥˜/IPMê³¼ì—´/Vccì €ì „ì••) ì£¼ìš” í˜„ìƒ IPM ë‚´ë¶€ì—ì„œì´ìƒê³¼ì „ë¥˜ê°ì§€ì‹œ, â€˜Error Displayâ€™ í›„ì œí’ˆì •ì§€ (ë°œìƒì‹œ, Error Level 3 / ì¼ì •íšŸìˆ˜ì´ìƒë°œìƒì‹œError Level 1) ê³ ê°ë¶ˆë§Œ ìš”ì†Œ CH\n",
            "\n",
            "ğŸ’¡ LLM ì„ íƒ ë²ˆí˜¸: 1\n",
            "\n",
            "================================================================================\n",
            "â¡ï¸ ITERATION 3 ë‹µë³€: **ì¡°ì¹˜:** CH21 ì••ì¶•ê¸° ì ê²€ ë° êµì²´\n",
            "================================================================================\n",
            "\n",
            "âœ… ëª¨ë“  ë¬¸ì„œë¥¼ ê²€ìƒ‰ ì™„ë£Œí–ˆìŠµë‹ˆë‹¤. ë°˜ë³µì„ ì¢…ë£Œí•©ë‹ˆë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1mrWag4MF8NL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Curation"
      ],
      "metadata": {
        "id": "0CfJyM77HeOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
      ],
      "metadata": {
        "id": "qUrQUxgsDoed"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
